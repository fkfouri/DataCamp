{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Natural Language Text\n",
    "## Chapter 2\n",
    "\n",
    "Uma analise de linguagem natural analisando um romance do Sherlock Homes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    !pip install pyspark==\"2.4.5\"  --quiet\n",
    "except:\n",
    "    print(\"Running throw py file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lower, col\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando a session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Analise Sherlock homes - Fabio Kfouri\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-Y520:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Analise Sherlock homes - Fabio Kfouri</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f9acf58c88>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read book\n",
    "df = spark.read.text(\"sherlock.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leitura da primeira linha da obra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(value=\"Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle\")\n"
     ]
    }
   ],
   "source": [
    "print(df.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantidade de linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12309\n"
     ]
    }
   ],
   "source": [
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizando um trecho da obra. Truncate setado como false permite a visualização de textos mais longos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------+\n",
      "|value                                                                       |\n",
      "+----------------------------------------------------------------------------+\n",
      "|Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle|\n",
      "|                                                                            |\n",
      "|This eBook is for the use of anyone anywhere at no cost and with            |\n",
      "|almost no restrictions whatsoever.  You may copy it, give it away or        |\n",
      "|re-use it under the terms of the Project Gutenberg License included         |\n",
      "|with this eBook or online at www.gutenberg.net                              |\n",
      "|                                                                            |\n",
      "|                                                                            |\n",
      "|Title: The Adventures of Sherlock Holmes                                    |\n",
      "|                                                                            |\n",
      "|Author: Arthur Conan Doyle                                                  |\n",
      "|                                                                            |\n",
      "|Release Date: November 29, 2002 [EBook #1661]                               |\n",
      "|Last Updated: May 20, 2019                                                  |\n",
      "|                                                                            |\n",
      "+----------------------------------------------------------------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(15, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformando em LowerCase e definido um Alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------+\n",
      "|value                                                                       |\n",
      "+----------------------------------------------------------------------------+\n",
      "|project gutenberg's the adventures of sherlock holmes, by arthur conan doyle|\n",
      "|                                                                            |\n",
      "|this ebook is for the use of anyone anywhere at no cost and with            |\n",
      "|almost no restrictions whatsoever.  you may copy it, give it away or        |\n",
      "|re-use it under the terms of the project gutenberg license included         |\n",
      "|with this ebook or online at www.gutenberg.net                              |\n",
      "|                                                                            |\n",
      "|                                                                            |\n",
      "|title: the adventures of sherlock holmes                                    |\n",
      "|                                                                            |\n",
      "|author: arthur conan doyle                                                  |\n",
      "|                                                                            |\n",
      "|release date: november 29, 2002 [ebook #1661]                               |\n",
      "|last updated: may 20, 2019                                                  |\n",
      "|                                                                            |\n",
      "+----------------------------------------------------------------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select(lower(col('value')).alias('value'))\n",
    "df.show(15, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing de textos e termos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(F.regexp_replace('value', 'Mr\\.', 'Mr').alias('value'))\n",
    "df = df.select(F.regexp_replace('value', 'don\\'t', 'do not').alias('value'))\n",
    "#df = df.select(F.regexp_replace('value', '\\'s', 'do not').alias('value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing Text, retorna uma matriz de sequencias de caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------+\n",
      "|words                                                                                        |\n",
      "+---------------------------------------------------------------------------------------------+\n",
      "|[project, gutenberg's, the, adventures, of, sherlock, holmes,, by, arthur, conan, doyle]     |\n",
      "|[]                                                                                           |\n",
      "|[this, ebook, is, for, the, use, of, anyone, anywhere, at, no, cost, and, with]              |\n",
      "|[almost, no, restrictions, whatsoever., , you, may, copy, it,, give, it, away, or]           |\n",
      "|[re-use, it, under, the, terms, of, the, project, gutenberg, license, included]              |\n",
      "|[with, this, ebook, or, online, at, www.gutenberg.net]                                       |\n",
      "|[]                                                                                           |\n",
      "|[]                                                                                           |\n",
      "|[title:, the, adventures, of, sherlock, holmes]                                              |\n",
      "|[]                                                                                           |\n",
      "|[author:, arthur, conan, doyle]                                                              |\n",
      "|[]                                                                                           |\n",
      "|[release, date:, november, 29,, 2002, [ebook, #1661]]                                        |\n",
      "|[last, updated:, may, 20,, 2019]                                                             |\n",
      "|[]                                                                                           |\n",
      "|[language:, english]                                                                         |\n",
      "|[]                                                                                           |\n",
      "|[character, set, encoding:, utf-8]                                                           |\n",
      "|[]                                                                                           |\n",
      "|[***, start, of, this, project, gutenberg, ebook, the, adventures, of, sherlock, holmes, ***]|\n",
      "+---------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1= df.select(F.split('Value', '[ ]').alias('words'))\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide o texto e remove simbolos indesejados, tais como pontuacao."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------+\n",
      "|words                                                                                              |\n",
      "+---------------------------------------------------------------------------------------------------+\n",
      "|[project, gutenberg, s, the, adventures, of, sherlock, holmes, , by, arthur, conan, doyle]         |\n",
      "|[]                                                                                                 |\n",
      "|[this, ebook, is, for, the, use, of, anyone, anywhere, at, no, cost, and, with]                    |\n",
      "|[almost, no, restrictions, whatsoever, , , you, may, copy, it, , give, it, away, or]               |\n",
      "|[re-use, it, under, the, terms, of, the, project, gutenberg, license, included]                    |\n",
      "|[with, this, ebook, or, online, at, www, gutenberg, net]                                           |\n",
      "|[]                                                                                                 |\n",
      "|[]                                                                                                 |\n",
      "|[title, , the, adventures, of, sherlock, holmes]                                                   |\n",
      "|[]                                                                                                 |\n",
      "|[author, , arthur, conan, doyle]                                                                   |\n",
      "|[]                                                                                                 |\n",
      "|[release, date, , november, 29, , 2002, , ebook, #1661, ]                                          |\n",
      "|[last, updated, , may, 20, , 2019]                                                                 |\n",
      "|[]                                                                                                 |\n",
      "|[language, , english]                                                                              |\n",
      "|[]                                                                                                 |\n",
      "|[character, set, encoding, , utf-8]                                                                |\n",
      "|[]                                                                                                 |\n",
      "|[, , , , start, of, this, project, gutenberg, ebook, the, adventures, of, sherlock, holmes, , , , ]|\n",
      "+---------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "punctuation = \"_|.\\?\\!\\\",\\'\\[\\]\\*():;<>”“’\"\n",
    "df2 = df.select(F.split('value', '[ %s]' % punctuation).alias('words'))\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explodindo em Array o campo words, colocando cada palavra em uma linha e preservando uma ordem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|   project|\n",
      "| gutenberg|\n",
      "|         s|\n",
      "|       the|\n",
      "|adventures|\n",
      "|        of|\n",
      "|  sherlock|\n",
      "|    holmes|\n",
      "|          |\n",
      "|        by|\n",
      "|    arthur|\n",
      "|     conan|\n",
      "|     doyle|\n",
      "|          |\n",
      "|      this|\n",
      "|     ebook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df2.select(F.explode('words').alias('word'))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tamanho de comparacao entre os dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12309 132720\n"
     ]
    }
   ],
   "source": [
    "print(df.count(), df3.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removendo colunas vazias (empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132720 108322\n"
     ]
    }
   ],
   "source": [
    "noblank_df = df3.where(F.length('word') > 0)\n",
    "print(df3.count(), noblank_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionando um ID no dataset usando a funcao Monotonically_Increasing_id(), criando uma coluna de números inteiros crescentes de forma eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|      word| id|\n",
      "+----------+---+\n",
      "|   project|  0|\n",
      "| gutenberg|  1|\n",
      "|         s|  2|\n",
      "|       the|  3|\n",
      "|adventures|  4|\n",
      "|        of|  5|\n",
      "|  sherlock|  6|\n",
      "|    holmes|  7|\n",
      "|        by|  8|\n",
      "|    arthur|  9|\n",
      "|     conan| 10|\n",
      "|     doyle| 11|\n",
      "|      this| 12|\n",
      "|     ebook| 13|\n",
      "|        is| 14|\n",
      "|       for| 15|\n",
      "|       the| 16|\n",
      "|       use| 17|\n",
      "|        of| 18|\n",
      "|    anyone| 19|\n",
      "+----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = noblank_df.select('word', F.monotonically_increasing_id().alias('id'))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particionando os dados\n",
    "O particionamento permite que o Spark paralelize operaçoes.\n",
    "\n",
    "No exemplo usa-se a função When em conjunto com a operação withColumn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.withColumn('title', F.when(df4.id < 25000, 'Preface')\n",
    "                             .when(df4.id < 50000, 'Chapter 1')\n",
    "                             .when(df4.id < 75000, 'Chapter 2')\n",
    "                             .otherwise('Chapter 3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-------+\n",
      "|      word| id|  title|\n",
      "+----------+---+-------+\n",
      "|   project|  0|Preface|\n",
      "| gutenberg|  1|Preface|\n",
      "|         s|  2|Preface|\n",
      "|       the|  3|Preface|\n",
      "|adventures|  4|Preface|\n",
      "|        of|  5|Preface|\n",
      "|  sherlock|  6|Preface|\n",
      "|    holmes|  7|Preface|\n",
      "|        by|  8|Preface|\n",
      "|    arthur|  9|Preface|\n",
      "|     conan| 10|Preface|\n",
      "|     doyle| 11|Preface|\n",
      "|      this| 12|Preface|\n",
      "|     ebook| 13|Preface|\n",
      "|        is| 14|Preface|\n",
      "|       for| 15|Preface|\n",
      "|       the| 16|Preface|\n",
      "|       use| 17|Preface|\n",
      "|        of| 18|Preface|\n",
      "|    anyone| 19|Preface|\n",
      "+----------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colocando uma nova coluna chamada part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df5.withColumn('part', F.when(df5.id < 25000, 0)\n",
    "                             .when(df5.id < 50000, 1)\n",
    "                             .when(df5.id < 75000, 2)\n",
    "                             .otherwise(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-------+----+\n",
      "|      word| id|  title|part|\n",
      "+----------+---+-------+----+\n",
      "|   project|  0|Preface|   0|\n",
      "| gutenberg|  1|Preface|   0|\n",
      "|         s|  2|Preface|   0|\n",
      "|       the|  3|Preface|   0|\n",
      "|adventures|  4|Preface|   0|\n",
      "|        of|  5|Preface|   0|\n",
      "|  sherlock|  6|Preface|   0|\n",
      "|    holmes|  7|Preface|   0|\n",
      "|        by|  8|Preface|   0|\n",
      "|    arthur|  9|Preface|   0|\n",
      "|     conan| 10|Preface|   0|\n",
      "|     doyle| 11|Preface|   0|\n",
      "|      this| 12|Preface|   0|\n",
      "|     ebook| 13|Preface|   0|\n",
      "|        is| 14|Preface|   0|\n",
      "|       for| 15|Preface|   0|\n",
      "|       the| 16|Preface|   0|\n",
      "|       use| 17|Preface|   0|\n",
      "|        of| 18|Preface|   0|\n",
      "|    anyone| 19|Preface|   0|\n",
      "+----------+---+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reparticiona os dados em df5, criando uma novo quadro de dados no df6 \"baseado em uma coluna\".\n",
    "\n",
    "<i>\"Coloque linhas com o mesmo valor da coluna 'part' na mesma partição\".</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 12\n"
     ]
    }
   ],
   "source": [
    "df6 = df5.repartition(12, 'part')\n",
    "print(df5.rdd.getNumPartitions(), df6.rdd.getNumPartitions(), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-------+----+\n",
      "|      word| id|  title|part|\n",
      "+----------+---+-------+----+\n",
      "|   project|  0|Preface|   0|\n",
      "| gutenberg|  1|Preface|   0|\n",
      "|         s|  2|Preface|   0|\n",
      "|       the|  3|Preface|   0|\n",
      "|adventures|  4|Preface|   0|\n",
      "|        of|  5|Preface|   0|\n",
      "|  sherlock|  6|Preface|   0|\n",
      "|    holmes|  7|Preface|   0|\n",
      "|        by|  8|Preface|   0|\n",
      "|    arthur|  9|Preface|   0|\n",
      "|     conan| 10|Preface|   0|\n",
      "|     doyle| 11|Preface|   0|\n",
      "|      this| 12|Preface|   0|\n",
      "|     ebook| 13|Preface|   0|\n",
      "|        is| 14|Preface|   0|\n",
      "|       for| 15|Preface|   0|\n",
      "|       the| 16|Preface|   0|\n",
      "|       use| 17|Preface|   0|\n",
      "|        of| 18|Preface|   0|\n",
      "|    anyone| 19|Preface|   0|\n",
      "+----------+---+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df6.coalesce(1).write.csv('spark_output/df6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving window analysis\n",
    "Uma tecnica em que se faz uma analise em conjunto de linhas (tupla de 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.createOrReplaceTempView(\"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----------+\n",
      "| id|        w1|        w2|        w3|\n",
      "+---+----------+----------+----------+\n",
      "|  0|   project| gutenberg|         s|\n",
      "|  1| gutenberg|         s|       the|\n",
      "|  2|         s|       the|adventures|\n",
      "|  3|       the|adventures|        of|\n",
      "|  4|adventures|        of|  sherlock|\n",
      "|  5|        of|  sherlock|    holmes|\n",
      "|  6|  sherlock|    holmes|        by|\n",
      "|  7|    holmes|        by|    arthur|\n",
      "|  8|        by|    arthur|     conan|\n",
      "|  9|    arthur|     conan|     doyle|\n",
      "| 10|     conan|     doyle|      this|\n",
      "| 11|     doyle|      this|     ebook|\n",
      "| 12|      this|     ebook|        is|\n",
      "| 13|     ebook|        is|       for|\n",
      "| 14|        is|       for|       the|\n",
      "| 15|       for|       the|       use|\n",
      "| 16|       the|       use|        of|\n",
      "| 17|       use|        of|    anyone|\n",
      "| 18|        of|    anyone|  anywhere|\n",
      "| 19|    anyone|  anywhere|        at|\n",
      "+---+----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "        SELECT id, word AS w1,\n",
    "               LEAD(word, 1) OVER(PARTITION BY part ORDER BY id) as w2,\n",
    "               LEAD(word, 2) OVER(PARTITION BY part ORDER BY id) as w3\n",
    "        FROM temp\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).sort('id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando agora o LAG, visualizamos as linhas anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----------+\n",
      "| id|        w1|        w2|        w3|\n",
      "+---+----------+----------+----------+\n",
      "|  0|      null|      null|   project|\n",
      "|  1|      null|   project| gutenberg|\n",
      "|  2|   project| gutenberg|         s|\n",
      "|  3| gutenberg|         s|       the|\n",
      "|  4|         s|       the|adventures|\n",
      "|  5|       the|adventures|        of|\n",
      "|  6|adventures|        of|  sherlock|\n",
      "|  7|        of|  sherlock|    holmes|\n",
      "|  8|  sherlock|    holmes|        by|\n",
      "|  9|    holmes|        by|    arthur|\n",
      "| 10|        by|    arthur|     conan|\n",
      "| 11|    arthur|     conan|     doyle|\n",
      "| 12|     conan|     doyle|      this|\n",
      "| 13|     doyle|      this|     ebook|\n",
      "| 14|      this|     ebook|        is|\n",
      "| 15|     ebook|        is|       for|\n",
      "| 16|        is|       for|       the|\n",
      "| 17|       for|       the|       use|\n",
      "| 18|       the|       use|        of|\n",
      "| 19|       use|        of|    anyone|\n",
      "+---+----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "        SELECT id, \n",
    "               LAG(word, 2) OVER(PARTITION BY part ORDER BY id) as w1,\n",
    "               LAG(word, 1) OVER(PARTITION BY part ORDER BY id) as w2,\n",
    "               word AS w3\n",
    "        FROM temp\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).sort('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+-------+\n",
      "|   id|     w1|     w2|     w3|\n",
      "+-----+-------+-------+-------+\n",
      "|50000|   null|   null|   case|\n",
      "|50001|   null|   case|against|\n",
      "|50002|   case|against|    you|\n",
      "|50003|against|    you|      i|\n",
      "|50004|    you|      i|     do|\n",
      "|50005|      i|     do|    not|\n",
      "|50006|     do|    not|   know|\n",
      "|50007|    not|   know|   that|\n",
      "|50008|   know|   that|  there|\n",
      "|50009|   that|  there|     is|\n",
      "|50010|  there|     is|    any|\n",
      "|50011|     is|    any| reason|\n",
      "|50012|    any| reason|   that|\n",
      "|50013| reason|   that|    the|\n",
      "|50014|   that|    the|details|\n",
      "|50015|    the|details| should|\n",
      "|50016|details| should|   find|\n",
      "|50017| should|   find|  their|\n",
      "|50018|   find|  their|    way|\n",
      "|50019|  their|    way|   into|\n",
      "+-----+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "        SELECT id, \n",
    "               LAG(word, 2) OVER(PARTITION BY part ORDER BY id) as w1,\n",
    "               LAG(word, 1) OVER(PARTITION BY part ORDER BY id) as w2,\n",
    "               word AS w3\n",
    "        FROM temp\n",
    "        WHERE part = 2\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).sort('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---------+---------+------------+------------+\n",
      "|part|       w1|       w2|       w3|          w4|          w5|\n",
      "+----+---------+---------+---------+------------+------------+\n",
      "|   3|     null|     null|  warning|           i|         had|\n",
      "|   3|     null|  warning|        i|         had|          so|\n",
      "|   3|  warning|        i|      had|          so|   foolishly|\n",
      "|   3|        i|      had|       so|   foolishly|    rejected|\n",
      "|   3|      had|       so|foolishly|    rejected|       ‘come|\n",
      "|   3|       so|foolishly| rejected|       ‘come|        come|\n",
      "|   3|foolishly| rejected|    ‘come|        come|         she|\n",
      "|   3| rejected|    ‘come|     come|         she|       cried|\n",
      "|   3|    ‘come|     come|      she|       cried|breathlessly|\n",
      "|   3|     come|      she|    cried|breathlessly|       ‘they|\n",
      "+----+---------+---------+---------+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "part,\n",
    "LAG(word, 2) OVER(PARTITION BY part ORDER BY id) AS w1,\n",
    "LAG(word, 1) OVER(PARTITION BY part ORDER BY id) AS w2,\n",
    "word AS w3,\n",
    "LEAD(word, 1) OVER(PARTITION BY part ORDER BY id) AS w4,\n",
    "LEAD(word, 2) OVER(PARTITION BY part ORDER BY id) AS w5\n",
    "FROM temp\n",
    "\"\"\"\n",
    "spark.sql(query).where(\"part = 3\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common word sequences.\n",
    "\n",
    "Como identificar as sequencias de palavras mais frequentes em um documento de texto em idioma natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+\n",
      "|   w1|   w2|   w3|count|\n",
      "+-----+-----+-----+-----+\n",
      "|  one|   of|  the|   48|\n",
      "|    i|think| that|   45|\n",
      "|   it|   is|    a|   45|\n",
      "|   it|  was|    a|   45|\n",
      "| that|   it|  was|   38|\n",
      "|  out|   of|  the|   35|\n",
      "| that|    i| have|   35|\n",
      "| that|   it|   is|   34|\n",
      "|    i|   do|  not|   34|\n",
      "|there|  was|    a|   34|\n",
      "| that|   he|  had|   30|\n",
      "| that|   he|  was|   30|\n",
      "| that|    i|  was|   28|\n",
      "| lord|   st|simon|   28|\n",
      "| that|    i|  had|   27|\n",
      "|   in|front|   of|   27|\n",
      "|    i| have|   no|   27|\n",
      "|think| that|    i|   26|\n",
      "|   to|   be|    a|   25|\n",
      "|    i|could|  not|   24|\n",
      "+-----+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query3agg = \"\"\"\n",
    "select w1, w2, w3, count(*) as count FROM (\n",
    "        SELECT id, word AS w1,\n",
    "               LEAD(word, 1) OVER(PARTITION BY part ORDER BY id) as w2,\n",
    "               LEAD(word, 2) OVER(PARTITION BY part ORDER BY id) as w3\n",
    "        FROM temp\n",
    "        )\n",
    "group by w1, w2, w3\n",
    "order by count desc\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query3agg).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma outra analise, analisando pelas tuplas mais longas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+------+\n",
      "|w1                 |w2                 |w3                 |length|\n",
      "+-------------------+-------------------+-------------------+------+\n",
      "|intellectual       |property           |trademark/copyright|39    |\n",
      "|comfortable-looking|building           |two-storied        |38    |\n",
      "|widespread         |comfortable-looking|building           |37    |\n",
      "|interesting        |character—dummy    |bell-ropes         |36    |\n",
      "|property           |trademark/copyright|agreement          |36    |\n",
      "|probability—the    |strong             |probability—is     |35    |\n",
      "|extraordinary      |circumstances      |connected          |35    |\n",
      "|simple-minded      |nonconformist      |clergyman          |35    |\n",
      "|particularly       |malignant          |boot-slitting      |34    |\n",
      "|especially         |commercial         |redistribution     |34    |\n",
      "|oppressively       |respectable        |frock-coat         |33    |\n",
      "|unsystematic       |sensational        |literature         |33    |\n",
      "|relentless         |keen-witted        |ready-handed       |33    |\n",
      "|redistributing     |project            |gutenberg-tm       |33    |\n",
      "|accomplishments    |‘my                |accomplishments    |33    |\n",
      "|intellectual       |property           |infringement       |32    |\n",
      "|ruddy-faced        |white-aproned      |landlord           |32    |\n",
      "|armitage—percy     |armitage—the       |second             |32    |\n",
      "|german-speaking    |country—in         |bohemia            |32    |\n",
      "|distributing       |performing         |displaying         |32    |\n",
      "+-------------------+-------------------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query3agg = \"\"\"\n",
    "select w1, w2, w3, length(w1) + length(w2) + length(w3) as length FROM (\n",
    "        SELECT id, word AS w1,\n",
    "               LEAD(word, 1) OVER(PARTITION BY part ORDER BY id) as w2,\n",
    "               LEAD(word, 2) OVER(PARTITION BY part ORDER BY id) as w3\n",
    "        FROM temp\n",
    "        )\n",
    "group by w1, w2, w3\n",
    "order by length desc\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query3agg).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontrando a maior sequencia por capitulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---+-----+-----+\n",
      "|  chapter|  w1| w2|   w3|count|\n",
      "+---------+----+---+-----+-----+\n",
      "|Chapter 1|that| he|  was|   16|\n",
      "|Chapter 2|  it|was|    a|   15|\n",
      "|Chapter 3|lord| st|simon|   28|\n",
      "|  Preface|  it| is|    a|   15|\n",
      "+---------+----+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query3agg = \"\"\"\n",
    "SELECT chapter, w1, w2, w3, count FROM\n",
    "(\n",
    "  SELECT\n",
    "  chapter,\n",
    "  ROW_NUMBER() OVER (PARTITION BY chapter ORDER BY chapter DESC) AS row,\n",
    "  w1, w2, w3, count\n",
    "  FROM ( --\n",
    "  \n",
    "          select chapter, w1, w2, w3, count(*) as count FROM (\n",
    "                SELECT id, title as chapter, word AS w1,\n",
    "                       LEAD(word, 1) OVER(PARTITION BY part ORDER BY id) as w2,\n",
    "                       LEAD(word, 2) OVER(PARTITION BY part ORDER BY id) as w3\n",
    "                FROM temp\n",
    "                )\n",
    "            group by chapter, w1, w2, w3\n",
    "            order by count desc --\n",
    "          )\n",
    "    )\n",
    "WHERE row = 1\n",
    "ORDER BY chapter ASC\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query3agg).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-d9c00751a19f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_cached\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'bool' object is not callable"
     ]
    }
   ],
   "source": [
    "df6.is_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.storageLevel.useMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'useMemory'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-8b6fee87bc1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetStorageLevel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0museMemory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'useMemory'"
     ]
    }
   ],
   "source": [
    "df.rdd.getStorageLevel.useMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
