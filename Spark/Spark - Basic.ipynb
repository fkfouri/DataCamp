{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkContext\n",
    "from pyspark import SparkContext as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.context.SparkContext'>\n"
     ]
    }
   ],
   "source": [
    "# Verify SparkContext\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<property object at 0x0000026726818958>\n"
     ]
    }
   ],
   "source": [
    "# Print Spark version\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create my_spark\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000026727E06608>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-Y520:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x26727e06608>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print my_spark\n",
    "print(spark)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Incluindo datasets\n",
    "spark.sparkContext.addFile('datasets/airports.csv')\n",
    "spark.sparkContext.addFile('datasets/flights_small.csv')\n",
    "spark.sparkContext.addFile('datasets/planes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Print the tables in the catalog\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "#read CSV Files\n",
    "airports = spark.read.csv(SparkFiles.get(\"airports.csv\"), header = True, sep = \",\")\n",
    "flights = spark.read.csv(SparkFiles.get(\"flights_small.csv\"), header = True, sep = \",\")\n",
    "planes = spark.read.csv(SparkFiles.get(\"planes.csv\"), header = True, sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|Project Gutenberg...|\n",
      "|                    |\n",
      "|This eBook is for...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read book\n",
    "sherlock = spark.read.text(\"sherlock\\sherlock.txt\")\n",
    "sherlock.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Views\n",
    "airports.createOrReplaceTempView(\"airports\")\n",
    "flights.createOrReplaceTempView(\"flights\")\n",
    "planes.createOrReplaceTempView(\"planes\")\n",
    "sherlock.createOrReplaceTempView(\"sherlock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='airports', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='flights', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='planes', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='sherlock', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "# Print the tables in the catalog\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"from flights select * limit 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first 10 rows \n",
    "temp = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
      "|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX| DEN|     121|     991|  10|    37|\n",
      "|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX| OAK|      90|     543|   8|    47|\n",
      "|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA| SFO|      98|     679|  16|    55|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a pandas DataFrame\n",
    "pd_temp = temp.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year month day dep_time dep_delay arr_time arr_delay carrier tailnum  \\\n",
      "0  2014    12   8      658        -7      935        -5      VX  N846VA   \n",
      "1  2014     1  22     1040         5     1505         5      AS  N559AS   \n",
      "2  2014     3   9     1443        -2     1652         2      VX  N847VA   \n",
      "3  2014     4   9     1705        45     1839        34      WN  N360SW   \n",
      "4  2014     3   9      754        -1     1015         1      AS  N612AS   \n",
      "\n",
      "  flight origin dest air_time distance hour minute  \n",
      "0   1780    SEA  LAX      132      954    6     58  \n",
      "1    851    SEA  HNL      360     2677   10     40  \n",
      "2    755    SEA  SFO      111      679   14     43  \n",
      "3    344    PDX  SJC       83      569   17      5  \n",
      "4    522    SEA  BUR      127      937    7     54  \n"
     ]
    }
   ],
   "source": [
    "# Print the head of pd_counts\n",
    "print(pd_temp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create pd_temp\n",
    "pd_temp = pd.DataFrame(np.random.random(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark_temp from pd_temp\n",
    "spark_temp = spark.createDataFrame(pd_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add spark_temp to the catalog\n",
    "spark_temp.createOrReplaceTempView(\"temp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|                  0|\n",
      "+-------------------+\n",
      "|  0.994825739301711|\n",
      "|0.07122966797125085|\n",
      "| 0.2948252020949441|\n",
      "| 0.4234781708603724|\n",
      "| 0.3350677030482747|\n",
      "|0.01011659380263219|\n",
      "|0.41466404118391265|\n",
      "|0.21541177682808155|\n",
      "| 0.7730650292317558|\n",
      "| 0.4540992168136757|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|              value|\n",
      "+-------------------+\n",
      "|  0.994825739301711|\n",
      "|0.07122966797125085|\n",
      "| 0.2948252020949441|\n",
      "| 0.4234781708603724|\n",
      "| 0.3350677030482747|\n",
      "|0.01011659380263219|\n",
      "|0.41466404118391265|\n",
      "|0.21541177682808155|\n",
      "| 0.7730650292317558|\n",
      "| 0.4540992168136757|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Renomeando coluna\n",
    "df = spark_temp.withColumn(\"value\", spark_temp[0]).select(\"value\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame flights\n",
    "flights = spark.table(\"flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: string (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: string (nullable = true)\n",
      " |-- distance: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: float (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: float (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "flights = flights.withColumn(\"distance\", flights[\"distance\"].cast('int'))\\\n",
    "                 .withColumn(\"air_time\", flights.air_time.cast('float'))\\\n",
    "                 .withColumn(\"dep_delay\", flights.dep_delay.cast('float'))\n",
    "\n",
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|     -7.0|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|   132.0|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|      5.0|    1505|        5|     AS| N559AS|   851|   SEA| HNL|   360.0|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|     -2.0|    1652|        2|     VX| N847VA|   755|   SEA| SFO|   111.0|     679|  14|    43|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|duration_hrs|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "|2014|   12|  8|     658|     -7.0|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|   132.0|     954|   6|    58|         2.2|\n",
      "|2014|    1| 22|    1040|      5.0|    1505|        5|     AS| N559AS|   851|   SEA| HNL|   360.0|    2677|  10|    40|         6.0|\n",
      "|2014|    3|  9|    1443|     -2.0|    1652|        2|     VX| N847VA|   755|   SEA| SFO|   111.0|     679|  14|    43|        1.85|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add duration_hrs\n",
    "flights = flights.withColumn(\"duration_hrs\", flights.air_time/60)\n",
    "flights.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL in a nutshell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+-------+\n",
      "| avg_time_duration|origin|carrier|\n",
      "+------------------+------+-------+\n",
      "| 2.641062801932367|   PDX|     US|\n",
      "|3.4815763052208837|   SEA|     AA|\n",
      "|2.5649012189995797|   PDX|     AS|\n",
      "|2.9572755417956658|   PDX|     DL|\n",
      "|1.5976558033161805|   SEA|     OO|\n",
      "|3.4336904761904763|   SEA|     B6|\n",
      "| 1.293419540229885|   PDX|     OO|\n",
      "|  2.78377065111759|   PDX|     UA|\n",
      "|3.3224926253687315|   SEA|     US|\n",
      "|2.1333333333333333|   PDX|     F9|\n",
      "|3.2710648148148147|   PDX|     AA|\n",
      "| 3.027322796934866|   SEA|     UA|\n",
      "|1.4189814814814814|   PDX|     VX|\n",
      "| 3.008447488584475|   PDX|     B6|\n",
      "|1.8805555555555555|   SEA|     VX|\n",
      "|1.7586513994910942|   PDX|     WN|\n",
      "| 3.136781609195402|   SEA|     DL|\n",
      "|5.6587962962962965|   SEA|     HA|\n",
      "|2.1505747126436785|   SEA|     F9|\n",
      "|2.6464069845533915|   SEA|     AS|\n",
      "+------------------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT AVG(air_time) / 60 avg_time_duration, origin, carrier\n",
    "FROM flights\n",
    "GROUP BY origin, carrier\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter flights by passing a string\n",
    "long_flights1 = flights.filter(\"distance > 1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter flights by passing a column of boolean values\n",
    "long_flights2 =  flights.filter(flights.distance > 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|duration_hrs|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "|2014|    1| 22|    1040|      5.0|    1505|        5|     AS| N559AS|   851|   SEA| HNL|   360.0|    2677|  10|    40|         6.0|\n",
      "|2014|    4| 19|    1236|     -4.0|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|   135.0|    1050|  12|    36|        2.25|\n",
      "|2014|   11| 19|    1812|     -3.0|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|   198.0|    1721|  18|    12|         3.3|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|duration_hrs|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "|2014|    1| 22|    1040|      5.0|    1505|        5|     AS| N559AS|   851|   SEA| HNL|   360.0|    2677|  10|    40|         6.0|\n",
      "|2014|    4| 19|    1236|     -4.0|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|   135.0|    1050|  12|    36|        2.25|\n",
      "|2014|   11| 19|    1812|     -3.0|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|   198.0|    1721|  18|    12|         3.3|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the data to check they're equal\n",
    "long_flights1.show(3)\n",
    "long_flights2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+\n",
      "|tailnum|origin|dest|\n",
      "+-------+------+----+\n",
      "| N846VA|   SEA| LAX|\n",
      "| N559AS|   SEA| HNL|\n",
      "| N847VA|   SEA| SFO|\n",
      "+-------+------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the first set of columns\n",
    "selected1 = flights.select(\"tailnum\", \"origin\", \"dest\")\n",
    "selected1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+\n",
      "|origin|dest|carrier|\n",
      "+------+----+-------+\n",
      "|   SEA| LAX|     VX|\n",
      "|   SEA| HNL|     AS|\n",
      "|   SEA| SFO|     VX|\n",
      "+------+----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the second set of columns\n",
    "temp = flights.select(flights.origin, flights.dest, flights.carrier)\n",
    "temp.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'(dest = PDX)'>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define first filter\n",
    "filterA = flights.origin == \"SEA\"\n",
    "\n",
    "# Define second filter\n",
    "filterB = flights.dest == \"PDX\"\n",
    "filterB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+\n",
      "|origin|dest|carrier|\n",
      "+------+----+-------+\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "|   SEA| PDX|     OO|\n",
      "+------+----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the data, first by filterA then by filterB\n",
    "selected2 = temp.filter(filterA).filter(filterB)\n",
    "selected2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'(distance / (air_time / 60)) AS `avg_speed`'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define avg_speed\n",
    "avg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n",
    "avg_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+------------------+\n",
      "|origin|dest|tailnum|         avg_speed|\n",
      "+------+----+-------+------------------+\n",
      "|   SEA| LAX| N846VA| 433.6363636363636|\n",
      "|   SEA| HNL| N559AS| 446.1666666666667|\n",
      "|   SEA| SFO| N847VA|367.02702702702703|\n",
      "+------+----+-------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the correct columns\n",
    "speed1 = flights.select(\"origin\", \"dest\", \"tailnum\", avg_speed)\n",
    "speed1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+------------------+\n",
      "|origin|dest|tailnum|         avg_speed|\n",
      "+------+----+-------+------------------+\n",
      "|   SEA| LAX| N846VA| 433.6363636363636|\n",
      "|   SEA| HNL| N559AS| 446.1666666666667|\n",
      "|   SEA| SFO| N847VA|367.02702702702703|\n",
      "+------+----+-------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the same table using a SQL expression\n",
    "speed2 = flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\")\n",
    "speed2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|min(distance)|\n",
      "+-------------+\n",
      "|          106|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the shortest flight from PDX in terms of distance\n",
    "flights.filter(flights.origin == \"PDX\").groupBy().min(\"distance\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|max(air_time)|\n",
      "+-------------+\n",
      "|        409.0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the longest flight from SEA in terms of air time\n",
    "flights.filter('origin = \"SEA\"').groupBy().max(\"air_time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     avg(air_time)|\n",
      "+------------------+\n",
      "|188.20689655172413|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average duration of Delta flights\n",
    "flights.filter(flights.carrier == 'DL').filter(flights.origin == 'SEA').groupBy().avg('air_time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "| sum(duration_hrs)|\n",
      "+------------------+\n",
      "|25289.600000000126|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total hours in the air\n",
    "flights.withColumn(\"duration_hrs\", flights.air_time/60).groupBy().sum(\"duration_hrs\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x2672583c488>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by tailnum\n",
    "by_plane = flights.groupBy(\"tailnum\")\n",
    "by_plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|tailnum|count|\n",
      "+-------+-----+\n",
      "| N442AS|   38|\n",
      "| N102UW|    2|\n",
      "| N36472|    4|\n",
      "+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of flights each plane made\n",
    "by_plane.count().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x26725847d48>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by origin\n",
    "by_origin = flights.groupBy(\"origin\")\n",
    "by_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|origin|     avg(air_time)|\n",
      "+------+------------------+\n",
      "|   SEA| 160.4361496051259|\n",
      "|   PDX|137.11543248288737|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average duration of flights from PDX and SEA\n",
    "by_origin.avg(\"air_time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by month and dest\n",
    "by_month_dest = flights.groupBy(\"month\", \"dest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------------------+\n",
      "|month|dest|     avg(dep_delay)|\n",
      "+-----+----+-------------------+\n",
      "|   11| TUS|-2.3333333333333335|\n",
      "|   11| ANC|  7.529411764705882|\n",
      "|    1| BUR|              -1.45|\n",
      "+-----+----+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average departure delay by month and destination\n",
    "by_month_dest.avg(\"dep_delay\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------------------+\n",
      "|month|dest|stddev_samp(dep_delay)|\n",
      "+-----+----+----------------------+\n",
      "|   11| TUS|    3.0550504633038935|\n",
      "|   11| ANC|    18.604716401245316|\n",
      "|    1| BUR|     15.22627576540667|\n",
      "+-----+----+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark.sql.functions as F\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Standard deviation of departure delay\n",
    "by_month_dest.agg(F.stddev(\"dep_delay\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[faa: string, name: string, lat: string, lon: string, alt: string, tz: string, dst: string]\n"
     ]
    }
   ],
   "source": [
    "# Examine the data\n",
    "print(airports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[dest: string, name: string, lat: string, lon: string, alt: string, tz: string, dst: string]\n"
     ]
    }
   ],
   "source": [
    "# Rename the faa column\n",
    "airports = airports.withColumnRenamed(\"faa\", \"dest\")\n",
    "print(airports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[year: string, month: string, day: string, dep_time: string, dep_delay: float, arr_time: string, arr_delay: string, carrier: string, tailnum: string, flight: string, origin: string, dest: string, air_time: float, distance: int, hour: string, minute: string, duration_hrs: double, dest: string, name: string, lat: string, lon: string, alt: string, tz: string, dst: string]\n"
     ]
    }
   ],
   "source": [
    "# Join the DataFrames\n",
    "flights_with_airports = flights.join(airports, flights.dest == airports.dest, how='left')\n",
    "print(flights_with_airports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+----+------------------+---------+-----------+---+---+---+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|duration_hrs|dest|              name|      lat|        lon|alt| tz|dst|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+----+------------------+---------+-----------+---+---+---+\n",
      "|2014|   12|  8|     658|     -7.0|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|   132.0|     954|   6|    58|         2.2| LAX|  Los Angeles Intl|33.942536|-118.408075|126| -8|  A|\n",
      "|2014|    1| 22|    1040|      5.0|    1505|        5|     AS| N559AS|   851|   SEA| HNL|   360.0|    2677|  10|    40|         6.0| HNL|     Honolulu Intl|21.318681|-157.922428| 13|-10|  N|\n",
      "|2014|    3|  9|    1443|     -2.0|    1652|        2|     VX| N847VA|   755|   SEA| SFO|   111.0|     679|  14|    43|        1.85| SFO|San Francisco Intl|37.618972|-122.374889| 13| -8|  A|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+------------+----+------------------+---------+-----------+---+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_with_airports.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+------------+------------------+---------+-----------+---+---+---+\n",
      "|dest|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|air_time|distance|hour|minute|duration_hrs|              name|      lat|        lon|alt| tz|dst|\n",
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+------------+------------------+---------+-----------+---+---+---+\n",
      "| LAX|2014|   12|  8|     658|     -7.0|     935|       -5|     VX| N846VA|  1780|   SEA|   132.0|     954|   6|    58|         2.2|  Los Angeles Intl|33.942536|-118.408075|126| -8|  A|\n",
      "| HNL|2014|    1| 22|    1040|      5.0|    1505|        5|     AS| N559AS|   851|   SEA|   360.0|    2677|  10|    40|         6.0|     Honolulu Intl|21.318681|-157.922428| 13|-10|  N|\n",
      "| SFO|2014|    3|  9|    1443|     -2.0|    1652|        2|     VX| N847VA|   755|   SEA|   111.0|     679|  14|    43|        1.85|San Francisco Intl|37.618972|-122.374889| 13| -8|  A|\n",
      "+----+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+--------+--------+----+------+------------+------------------+---------+-----------+---+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the DataFrames\n",
    "flights_with_airports = flights.join(airports, on= [\"dest\"], how='leftouter')\n",
    "flights_with_airports.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename year column\n",
    "planes = planes.withColumnRenamed('year', 'plane_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+------------+----------+--------------------+------------+--------+-------+-----+-----+---------+\n",
      "|tailnum|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|flight|origin|dest|air_time|distance|hour|minute|duration_hrs|plane_year|                type|manufacturer|   model|engines|seats|speed|   engine|\n",
      "+-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+------------+----------+--------------------+------------+--------+-------+-----+-----+---------+\n",
      "| N846VA|2014|   12|  8|     658|     -7.0|     935|       -5|     VX|  1780|   SEA| LAX|   132.0|     954|   6|    58|         2.2|      2011|Fixed wing multi ...|      AIRBUS|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "| N559AS|2014|    1| 22|    1040|      5.0|    1505|        5|     AS|   851|   SEA| HNL|   360.0|    2677|  10|    40|         6.0|      2006|Fixed wing multi ...|      BOEING| 737-890|      2|  149|   NA|Turbo-fan|\n",
      "| N847VA|2014|    3|  9|    1443|     -2.0|    1652|        2|     VX|   755|   SEA| SFO|   111.0|     679|  14|    43|        1.85|      2011|Fixed wing multi ...|      AIRBUS|A320-214|      2|  182|   NA|Turbo-fan|\n",
      "+-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+------------+----------+--------------------+------------+--------+-------+-----+-----+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: float (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- flight: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: float (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      " |-- duration_hrs: double (nullable = true)\n",
      " |-- plane_year: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- engines: string (nullable = true)\n",
      " |-- seats: string (nullable = true)\n",
      " |-- speed: string (nullable = true)\n",
      " |-- engine: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the DataFrames\n",
    "model_data = flights.join(planes, on='tailnum', how=\"leftouter\")\n",
    "model_data.show(3)\n",
    "model_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: float (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: integer (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- flight: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      " |-- duration_hrs: double (nullable = true)\n",
      " |-- plane_year: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- engines: string (nullable = true)\n",
      " |-- seats: string (nullable = true)\n",
      " |-- speed: string (nullable = true)\n",
      " |-- engine: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cast the columns to integers\n",
    "model_data = model_data.withColumn(\"arr_delay\", model_data.arr_delay.cast('integer'))\n",
    "model_data = model_data.withColumn(\"air_time\", model_data.air_time.cast('integer'))\n",
    "model_data = model_data.withColumn(\"month\", model_data.month.cast('integer'))\n",
    "model_data = model_data.withColumn(\"plane_year\", model_data.plane_year.cast('integer'))\n",
    "model_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9303"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the column plane_age\n",
    "model_data = model_data.withColumn(\"plane_age\", model_data.year - model_data.plane_year)\n",
    "model_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9303"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create is_late\n",
    "model_data = model_data.withColumn(\"is_late\", model_data.arr_delay > 0)\n",
    "\n",
    "# Convert to an integer\n",
    "model_data = model_data.withColumn(\"label\", model_data.is_late.cast('integer'))\n",
    "\n",
    "# Remove missing values\n",
    "model_data = model_data.filter(\"arr_delay is not NULL \\\n",
    "                                and dep_delay is not NULL \\\n",
    "                                and air_time is not NULL \\\n",
    "                                and plane_year is not NULL\")\n",
    "model_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical features\n",
    "Como você sabe, o Spark requer dados numéricos para modelagem. Até agora, isso não foi um problema; mesmo colunas booleanas podem ser facilmente convertidas em números inteiros sem nenhum problema. Mas você também estará usando a companhia aérea e o destino do avião como recursos em seu modelo. Eles são codificados como strings e não há nenhuma maneira óbvia de convertê-los em um tipo de dados numérico. \n",
    "\n",
    "Felizmente, o PySpark possui funções para lidar com isso incorporado ao submódulo pyspark.ml.features. Você pode criar o que é chamado de 'vetores quentes' para representar a transportadora e o destino de cada voo. Um vetor quente é uma maneira de representar uma característica categórica em que toda observação tem um vetor no qual todos os elementos são zero, exceto no máximo um elemento, que tem o valor de um (1). Cada elemento no vetor corresponde a um nível do recurso, portanto, é possível dizer qual é o nível certo, vendo qual elemento do vetor é igual a um (1). \n",
    "\n",
    "O primeiro passo para codificar seu recurso categórico é criar um StringIndexer. Os membros desta classe são estimadores que usam um DataFrame com uma coluna de cadeias e mapeiam cada cadeia exclusiva para um número. Em seguida, o Estimator retorna um Transformer que pega um DataFrame, anexa o mapeamento a ele como metadados e retorna um novo DataFrame com uma coluna numérica correspondente à coluna da string. \n",
    "\n",
    "A segunda etapa é codificar essa coluna numérica como um vetor quente usando um OneHotEncoder. Isso funciona exatamente da mesma maneira que o StringIndexer, criando um estimador e depois um transformador. O resultado final é uma coluna que codifica seu recurso categórico como um vetor adequado para rotinas de aprendizado de máquina! \n",
    "\n",
    "Isso pode parecer complicado, mas não se preocupe! Tudo o que você precisa lembrar é que você precisa criar um StringIndexer e um OneHotEncoder, e o Pipeline cuidará do resto.\n",
    "\n",
    "http://spark.apache.org/docs/latest/ml-features.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# Create a StringIndexer\n",
    "carr_indexer = StringIndexer(inputCol=\"carrier\", outputCol=\"carrier_index\")\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "carr_encoder = OneHotEncoder(inputCol=\"carrier_index\", outputCol=\"carrier_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StringIndexer\n",
    "dest_indexer = StringIndexer(inputCol = \"dest\", outputCol = \"dest_index\")\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "dest_encoder = OneHotEncoder(inputCol = \"dest_index\", outputCol = \"dest_fact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A última etapa do pipeline é combinar todas as colunas que contêm nossos recursos em uma única coluna. Isso deve ser feito antes que a modelagem possa ocorrer, pois toda rotina de modelagem do Spark espera que os dados estejam neste formulário. Você pode fazer isso armazenando cada um dos valores de uma coluna como uma entrada em um vetor. Então, do ponto de vista do modelo, toda observação é um vetor que contém todas as informações sobre ele e um rótulo que informa ao modelador a que valor essa observação corresponde. \n",
    "\n",
    "Por esse motivo, o submódulo pyspark.ml.feature contém uma classe chamada VectorAssembler. Este Transformer pega todas as colunas especificadas e as combina em uma nova coluna vetorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Make a VectorAssembler\n",
    "vec_assembler = VectorAssembler(inputCols=[\"month\", \"air_time\", \"carrier_fact\", \"dest_fact\", \"plane_age\"], \\\n",
    "                                outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você está finalmente pronto para criar um pipeline!\n",
    "\n",
    "Pipeline é uma classe no módulo pyspark.ml que combina todos os estimadores e transformadores que você já criou. Isso permite reutilizar o mesmo processo de modelagem repetidamente, agrupando-o em um objeto simples. Legal, certo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Make the pipeline\n",
    "flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste e Treinamento\n",
    "\n",
    "Depois que você limpa seus dados e os prepara para modelagem, uma das etapas mais importantes é dividir os dados em um conjunto de testes e um conjunto de treinamento. Depois disso, não toque nos dados do teste até achar que tem um bom modelo! Ao criar modelos e formar hipóteses, você pode testá-los em seus dados de treinamento para ter uma idéia do desempenho deles. \n",
    "\n",
    "Depois de obter seu modelo favorito, você poderá ver quão bem ele prevê os novos dados no seu conjunto de testes. Esses dados nunca antes vistos darão uma idéia muito mais realista do desempenho do seu modelo no mundo real quando você estiver tentando prever ou classificar novos dados. \n",
    "\n",
    "No Spark, é importante garantir que você divida os dados após todas as transformações. Isso ocorre porque operações como StringIndexer nem sempre produzem o mesmo índice, mesmo quando recebem a mesma lista de strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+------------+----------+--------------------+------------+--------+-------+-----+-----+---------+---------+-------+-----+----------+---------------+-------------+--------------+--------------------+\n",
      "|tailnum|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|flight|origin|dest|air_time|distance|hour|minute|duration_hrs|plane_year|                type|manufacturer|   model|engines|seats|speed|   engine|plane_age|is_late|label|dest_index|      dest_fact|carrier_index|  carrier_fact|            features|\n",
      "+-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+------------+----------+--------------------+------------+--------+-------+-----+-----+---------+---------+-------+-----+----------+---------------+-------------+--------------+--------------------+\n",
      "| N846VA|2014|   12|  8|     658|     -7.0|     935|       -5|     VX|  1780|   SEA| LAX|     132|     954|   6|    58|         2.2|      2011|Fixed wing multi ...|      AIRBUS|A320-214|      2|  182|   NA|Turbo-fan|      3.0|  false|    0|       1.0| (68,[1],[1.0])|          7.0|(10,[7],[1.0])|(81,[0,1,9,13,80]...|\n",
      "| N559AS|2014|    1| 22|    1040|      5.0|    1505|        5|     AS|   851|   SEA| HNL|     360|    2677|  10|    40|         6.0|      2006|Fixed wing multi ...|      BOEING| 737-890|      2|  149|   NA|Turbo-fan|      8.0|   true|    1|      19.0|(68,[19],[1.0])|          0.0|(10,[0],[1.0])|(81,[0,1,2,31,80]...|\n",
      "| N847VA|2014|    3|  9|    1443|     -2.0|    1652|        2|     VX|   755|   SEA| SFO|     111|     679|  14|    43|        1.85|      2011|Fixed wing multi ...|      AIRBUS|A320-214|      2|  182|   NA|Turbo-fan|      3.0|   true|    1|       0.0| (68,[0],[1.0])|          7.0|(10,[7],[1.0])|(81,[0,1,9,12,80]...|\n",
      "+-------+----+-----+---+--------+---------+--------+---------+-------+------+------+----+--------+--------+----+------+------------+----------+--------------------+------------+--------+-------+-----+-----+---------+---------+-------+-----+----------+---------------+-------------+--------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the data\n",
    "piped_data = flights_pipe.fit(model_data).transform(model_data)\n",
    "\n",
    "piped_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5621 3682\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets with same Seed\n",
    "# 60% training and 40% test\n",
    "training, test = piped_data.randomSplit([.6, .4], 42)\n",
    "print(training.count(), test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Logística\n",
    "\n",
    "O modelo que você ajustará neste capítulo é chamado de regressão logística. Este modelo é muito semelhante a uma regressão linear, mas, em vez de prever uma variável numérica, prevê a probabilidade (entre 0 e 1) de um evento. \n",
    "\n",
    "Para usar isso como um algoritmo de classificação, basta atribuir um ponto de corte a essas probabilidades. Se a probabilidade prevista estiver acima do ponto de corte, você classifica essa observação como um 'sim' (neste caso, o vôo está atrasado); se estiver abaixo, você a classifica como um 'não'! \n",
    "\n",
    "Você ajustará esse modelo testando valores diferentes para vários hiperparâmetros. Um hiperparâmetro é apenas um valor no modelo que não é estimado a partir dos dados, mas é fornecido pelo usuário para maximizar o desempenho do modelo. Para este curso, não é necessário entender a matemática por trás de todos esses valores - o importante é que você experimente algumas opções diferentes e escolha a melhor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LogisticRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a LogisticRegression Estimator\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora você ajustará seu modelo de regressão logística usando um procedimento chamado validação cruzada k-fold. Este é um método de estimar o desempenho do modelo em dados não vistos (como o DataFrame de teste).\n",
    "\n",
    "Ele funciona dividindo os dados de treinamento em algumas partições diferentes. O número exato depende de você, mas neste curso você usará o valor padrão de três do PySpark. Depois que os dados são divididos, uma das partições é retirada e o modelo é adequado para as outras. Em seguida, o erro é medido em relação à partição retida. Isso é repetido para cada uma das partições, para que cada bloco de dados seja mantido e usado como um conjunto de testes exatamente uma vez. Em seguida, é calculado o erro médio em cada uma das partições. Isso é chamado de erro de validação cruzada do modelo e é uma boa estimativa do erro real nos dados retidos. \n",
    "\n",
    "Você usará a validação cruzada para escolher os hiperparâmetros criando uma grade dos possíveis pares de valores para os dois hiperparâmetros, elasticNetParam e regParam, e usando o erro de validação cruzada para comparar todos os modelos diferentes para escolher o melhor!\n",
    "\n",
    "O que a validação cruzada permite estimar? O Erro estimado do modelo no conjunto de testes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie o avaliador\n",
    "\n",
    "A primeira coisa que você precisa ao fazer a validação cruzada para a seleção de modelos é uma maneira de comparar diferentes modelos. Felizmente, o submódulo pyspark.ml.evaluation possui classes para avaliar diferentes tipos de modelos. Seu modelo é um modelo de classificação binária; portanto, você usará o BinaryClassificationEvaluator no módulo pyspark.ml.evaluation.\n",
    "\n",
    "Este avaliador calcula a área sob o ROC. Essa é uma métrica que combina os dois tipos de erros que um classificador binário pode cometer (falsos positivos e falsos negativos) em um número simples. Você aprenderá mais sobre isso no final do capítulo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluation submodule\n",
    "import pyspark.ml.evaluation as evals\n",
    "\n",
    "# Create a BinaryClassificationEvaluator\n",
    "evaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crie uma grid\n",
    "\n",
    "Em seguida, você precisa criar uma grade de valores para pesquisar ao procurar os hiperparâmetros ideais. O submódulo pyspark.ml.tuning inclui uma classe chamada ParamGridBuilder que faz exatamente isso (talvez você esteja começando a perceber um padrão aqui; o PySpark possui um submódulo para praticamente tudo!).\n",
    "\n",
    "Você precisará usar os métodos .addGrid() e .build() para criar uma grade que possa ser usada para validação cruzada. O método .addGrid() usa um parâmetro de modelo (um atributo do modelo Estimator, lr (regressao Logistica), que você criou há alguns exercícios atrás) e uma lista de valores que você deseja tentar. O método .build () não aceita argumentos, apenas retorna a grade que você usará mais tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tuning submodule\n",
    "import pyspark.ml.tuning as tune\n",
    "import numpy as np\n",
    "\n",
    "# Create the parameter grid\n",
    "grid = tune.ParamGridBuilder()\n",
    "\n",
    "# Add the hyperparameter\n",
    "# numpy cria uma lista de numeros de 0 a 0.1 incrementando por .01\n",
    "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01)) \n",
    "grid = grid.addGrid(lr.elasticNetParam, [0, 1])\n",
    "\n",
    "# Build the grid\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crie um validador\n",
    "\n",
    "O submódulo pyspark.ml.tuning também possui uma classe chamada CrossValidator para executar a validação cruzada. Esse Estimador leva o modelador que você deseja ajustar, a grade de hiperparâmetros criados e o avaliador que deseja usar para comparar seus modelos.\n",
    "\n",
    "O submódulo pyspark.ml.tune já foi importado como sintonia. Você criará o CrossValidator passando o Estimador de regressão logística lr, a grade de parâmetros e o avaliador que você criou nos exercícios anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CrossValidator\n",
    "cv = tune.CrossValidator(estimator=lr,\n",
    "               estimatorParamMaps=grid,\n",
    "               evaluator=evaluator )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste o modelo\n",
    "\n",
    "Você está finalmente pronto para encaixar nos modelos e selecionar o melhor!\n",
    "\n",
    "Infelizmente, a validação cruzada é um procedimento muito intensivo em termos computacionais. A instalação de todos os modelos levaria muito tempo no DataCamp.\n",
    "\n",
    "Para fazer isso localmente, você usaria o código:\n",
    "\n",
    "```\n",
    "# Ajustar modelos de validação cruzada\n",
    "modelos = cv.fit (treinamento)\n",
    "\n",
    "# Extraia o melhor modelo\n",
    "best_lr = models.bestModel\n",
    "```\n",
    "\n",
    "Lembre-se de que os dados de treinamento são chamados de treinamento e você está usando lr para ajustar-se a um modelo de regressão logística. A validação cruzada selecionou os valores dos parâmetros regParam = 0 e elasticNetParam = 0 como os melhores. Esses são os valores padrão, portanto, você não precisa fazer mais nada com lr antes de ajustar o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel: uid = LogisticRegression_c9659a62fe2b, numClasses = 2, numFeatures = 81\n"
     ]
    }
   ],
   "source": [
    "# Call lr.fit()\n",
    "best_lr = lr.fit(training)\n",
    "\n",
    "# Print best_lr\n",
    "print(best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit cross validation models\n",
    "models = cv.fit(training)\n",
    "\n",
    "# Extract the best model\n",
    "best_lr = models.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel: uid = LogisticRegression_c9659a62fe2b, numClasses = 2, numFeatures = 81\n"
     ]
    }
   ],
   "source": [
    "# Print best_lr\n",
    "print(best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossValidatorModel_5b4b685e8cbd"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliando classificadores binários\n",
    "\n",
    "Neste curso, usaremos uma métrica comum para algoritmos de classificação binária chamada AUC, ou área sob a curva. Nesse caso, a curva é a ROC, ou curva de operação do receptor. Os detalhes do que essas coisas realmente medem não são importantes para este curso. Tudo o que você precisa saber é que, para nossos propósitos, quanto mais próxima a AUC estiver de uma (1), melhor o modelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.699089985568893\n"
     ]
    }
   ],
   "source": [
    "# Use the model to predict the test set\n",
    "test_results = best_lr.transform(test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "print(evaluator.evaluate(test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
